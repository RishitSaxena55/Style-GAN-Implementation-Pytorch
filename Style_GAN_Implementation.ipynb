{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMYeYHveUPf"
      },
      "source": [
        "# **Style-GAN-Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "R9fxklHqxfqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zItDmeQUeUPh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import IPython\n",
        "\n",
        "!pip install -q torchinfo\n",
        "import torchinfo\n",
        "from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Implementations**"
      ],
      "metadata": {
        "id": "YnMXptABxkGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Layer"
      ],
      "metadata": {
        "id": "sqDYXKipxrCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hcun8uxKeUPi"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n",
        "        super().__init__()\n",
        "        he_std = gain * input_size**(-0.5) # He init\n",
        "        # Equalized learning rate and custom learning rate multiplier.\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "        return F.linear(x, self.weight * self.w_mul, bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLSNlbFoeUPi"
      },
      "source": [
        "### Convolution Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FCxq80nleUPj"
      },
      "outputs": [],
      "source": [
        "class MyConv2d(nn.Module):\n",
        "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n",
        "                intermediate=None, upscale=False):\n",
        "        super().__init__()\n",
        "        if upscale:\n",
        "            self.upscale = Upscale2d()\n",
        "        else:\n",
        "            self.upscale = None\n",
        "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n",
        "        self.kernel_size = kernel_size\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.intermediate = intermediate\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "\n",
        "        have_convolution = False\n",
        "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
        "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
        "            # this really needs to be cleaned up and go into the conv...\n",
        "            w = self.weight * self.w_mul\n",
        "            w = w.permute(1, 0, 2, 3)\n",
        "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
        "            w = F.pad(w, (1,1,1,1))\n",
        "            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
        "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
        "            have_convolution = True\n",
        "        elif self.upscale is not None:\n",
        "            x = self.upscale(x)\n",
        "\n",
        "        if not have_convolution and self.intermediate is None:\n",
        "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n",
        "        elif not have_convolution:\n",
        "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
        "\n",
        "        if self.intermediate is not None:\n",
        "            x = self.intermediate(x)\n",
        "        if bias is not None:\n",
        "            x = x + bias.view(1, -1, 1, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUgA9DO5eUPj"
      },
      "source": [
        "### Noise Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "S9N-VQA_eUPj"
      },
      "outputs": [],
      "source": [
        "class NoiseLayer(nn.Module):\n",
        "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(channels))\n",
        "        self.noise = None\n",
        "\n",
        "    def forward(self, x, noise=None):\n",
        "        if noise is None and self.noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
        "        elif noise is None:\n",
        "            # here is a little trick: if you get all the noiselayers and set each\n",
        "            # modules .noise attribute, you can have pre-defined noise.\n",
        "            # Very useful for analysis\n",
        "            noise = self.noise\n",
        "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9BjD-DeUPj"
      },
      "source": [
        "### Style Modification layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5baylXpieUPk"
      },
      "outputs": [],
      "source": [
        "class StyleMod(nn.Module):\n",
        "    def __init__(self, latent_size, channels, use_wscale):\n",
        "        super(StyleMod, self).__init__()\n",
        "        self.lin = MyLinear(latent_size,\n",
        "                            channels * 2,\n",
        "                            gain=1.0, use_wscale=use_wscale)\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        style = self.lin(latent) # style => [batch_size, n_channels*2]\n",
        "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
        "        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
        "        x = x * (style[:, 0] + 1.) + style[:, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D6GxZpFeUPk"
      },
      "source": [
        "### Pixelnorm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u-leqKKxeUPk"
      },
      "outputs": [],
      "source": [
        "class PixelNormLayer(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2UxW2ABeUPk"
      },
      "source": [
        "# Upscale and blur layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ep9kqEzVeUPk"
      },
      "outputs": [],
      "source": [
        "class BlurLayer(nn.Module):\n",
        "    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n",
        "        super(BlurLayer, self).__init__()\n",
        "        kernel=[1, 2, 1]\n",
        "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
        "        kernel = kernel[:, None] * kernel[None, :]\n",
        "        kernel = kernel[None, None]\n",
        "        if normalize:\n",
        "            kernel = kernel / kernel.sum()\n",
        "        if flip:\n",
        "            kernel = kernel[:, :, ::-1, ::-1]\n",
        "        self.register_buffer('kernel', kernel)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # expand kernel channels\n",
        "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
        "        x = F.conv2d(\n",
        "            x,\n",
        "            kernel,\n",
        "            stride=self.stride,\n",
        "            padding=int((self.kernel.size(2)-1)/2),\n",
        "            groups=x.size(1)\n",
        "        )\n",
        "        return x\n",
        "\n",
        "def upscale2d(x, factor=2, gain=1):\n",
        "    assert x.dim() == 4\n",
        "    if gain != 1:\n",
        "        x = x * gain\n",
        "    if factor != 1:\n",
        "        shape = x.shape\n",
        "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
        "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
        "    return x\n",
        "\n",
        "class Upscale2d(nn.Module):\n",
        "    def __init__(self, factor=2, gain=1):\n",
        "        super().__init__()\n",
        "        assert isinstance(factor, int) and factor >= 1\n",
        "        self.gain = gain\n",
        "        self.factor = factor\n",
        "    def forward(self, x):\n",
        "        return upscale2d(x, factor=self.factor, gain=self.gain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxszIvm6eUPk"
      },
      "source": [
        "### Generator Mapping Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4OwRgGpgeUPl"
      },
      "outputs": [],
      "source": [
        "class G_mapping(nn.Sequential):\n",
        "    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        layers = [\n",
        "            ('pixel_norm', PixelNormLayer()),\n",
        "            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense0_act', act),\n",
        "            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense1_act', act),\n",
        "            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense2_act', act),\n",
        "            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense3_act', act),\n",
        "            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense4_act', act),\n",
        "            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense5_act', act),\n",
        "            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense6_act', act),\n",
        "            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense7_act', act)\n",
        "        ]\n",
        "        super().__init__(OrderedDict(layers))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        # Broadcast\n",
        "        x = x.unsqueeze(1).expand(-1, 18, -1)\n",
        "        return x\n",
        "\n",
        "class Truncation(nn.Module):\n",
        "    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n",
        "        super().__init__()\n",
        "        self.max_layer = max_layer\n",
        "        self.threshold = threshold\n",
        "        self.register_buffer('avg_latent', avg_latent)\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 3\n",
        "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
        "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n",
        "        return torch.where(do_trunc, interp, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlQ-lSgIeUPl"
      },
      "source": [
        "### Generator Synthesis Blocks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qPFgGua8eUPl"
      },
      "outputs": [],
      "source": [
        "class LayerEpilogue(nn.Module):\n",
        "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
        "    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        if use_noise:\n",
        "            layers.append(('noise', NoiseLayer(channels)))\n",
        "        layers.append(('activation', activation_layer))\n",
        "        if use_pixel_norm:\n",
        "            layers.append(('pixel_norm', PixelNorm()))\n",
        "        if use_instance_norm:\n",
        "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
        "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
        "        if use_styles:\n",
        "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
        "        else:\n",
        "            self.style_mod = None\n",
        "    def forward(self, x, dlatents_in_slice=None):\n",
        "        x = self.top_epi(x)\n",
        "        if self.style_mod is not None:\n",
        "            x = self.style_mod(x, dlatents_in_slice)\n",
        "        else:\n",
        "            assert dlatents_in_slice is None\n",
        "        return x\n",
        "\n",
        "\n",
        "class InputBlock(nn.Module):\n",
        "    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        super().__init__()\n",
        "        self.const_input_layer = const_input_layer\n",
        "        self.nf = nf\n",
        "        if self.const_input_layer:\n",
        "            # called 'const' in tf\n",
        "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
        "            self.bias = nn.Parameter(torch.ones(nf))\n",
        "        else:\n",
        "            self.dense = MyLinear(dlatent_size, nf*16, gain=gain/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n",
        "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "\n",
        "    def forward(self, dlatents_in_range):\n",
        "        batch_size = dlatents_in_range.size(0)\n",
        "        if self.const_input_layer:\n",
        "            x = self.const.expand(batch_size, -1, -1, -1)\n",
        "            x = x + self.bias.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
        "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        x = self.conv(x)\n",
        "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        return x\n",
        "\n",
        "\n",
        "class GSynthesisBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        # 2**res x 2**res # res = 3..resolution_log2\n",
        "        super().__init__()\n",
        "        if blur_filter:\n",
        "            blur = BlurLayer(blur_filter)\n",
        "        else:\n",
        "            blur = None\n",
        "        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
        "                                 intermediate=blur, upscale=True)\n",
        "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "\n",
        "    def forward(self, x, dlatents_in_range):\n",
        "        x = self.conv0_up(x)\n",
        "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        x = self.conv1(x)\n",
        "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_39sAL_eUPl"
      },
      "source": [
        "# Generator - Synthesis part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pyTiGAWTeUPl"
      },
      "outputs": [],
      "source": [
        "class G_synthesis(nn.Module):\n",
        "    def __init__(self,\n",
        "        dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n",
        "        num_channels        = 3,            # Number of output color channels.\n",
        "        resolution          = 1024,         # Output resolution.\n",
        "        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
        "        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
        "        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
        "        use_styles          = True,         # Enable style inputs?\n",
        "        const_input_layer   = True,         # First layer is a learned constant?\n",
        "        use_noise           = True,         # Enable noise inputs?\n",
        "        randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
        "        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n",
        "        use_wscale          = True,         # Enable equalized learning rate?\n",
        "        use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n",
        "        use_instance_norm   = True,         # Enable instance normalization?\n",
        "        dtype               = torch.float32,  # Data type to use for activations and outputs.\n",
        "        blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "        ):\n",
        "\n",
        "        super().__init__()\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "        self.dlatent_size = dlatent_size\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2**resolution_log2 and resolution >= 4\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        num_layers = resolution_log2 * 2 - 2\n",
        "        num_styles = num_layers if use_styles else 1\n",
        "        torgbs = []\n",
        "        blocks = []\n",
        "        for res in range(2, resolution_log2 + 1):\n",
        "            channels = nf(res-1)\n",
        "            name = '{s}x{s}'.format(s=2**res)\n",
        "            if res == 2:\n",
        "                blocks.append((name,\n",
        "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
        "                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "\n",
        "            else:\n",
        "                blocks.append((name,\n",
        "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "            last_channels = channels\n",
        "        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n",
        "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
        "\n",
        "    def forward(self, dlatents_in):\n",
        "        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n",
        "        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n",
        "        batch_size = dlatents_in.size(0)\n",
        "        for i, m in enumerate(self.blocks.values()):\n",
        "            if i == 0:\n",
        "                x = m(dlatents_in[:, 2*i:2*i+2])\n",
        "            else:\n",
        "                x = m(x, dlatents_in[:, 2*i:2*i+2])\n",
        "        rgb = self.torgb(x)\n",
        "        return rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbtjDr1eUPl"
      },
      "source": [
        "## All done, let's define the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Bnf7UbBPeUPl"
      },
      "outputs": [],
      "source": [
        "g_all = nn.Sequential(OrderedDict([\n",
        "    ('g_mapping', G_mapping()),\n",
        "    #('truncation', Truncation(avg_latent)),\n",
        "    ('g_synthesis', G_synthesis())\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU8EreFteUPl"
      },
      "source": [
        "### Downloading Pytorch-ready pre-trained weights from:\n",
        "(https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sctsZqB5eUPm"
      },
      "source": [
        "Let's load the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vOdSLJ8eUPm",
        "outputId": "4242839b-a91f-4fa0-d2b6-3cb536ee2307"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "g_all.load_state_dict(torch.load('./karras2019stylegan-ffhq-1024x1024.for_g_all.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5BvA3BeUPm"
      },
      "source": [
        "Now we're all set! Let's generate faces!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXh0HrRLeUPm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot\n",
        "import torchvision\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "g_all.eval()\n",
        "g_all.to(device)\n",
        "\n",
        "nb_rows = 2\n",
        "nb_cols = 5\n",
        "nb_samples = nb_rows * nb_cols\n",
        "latents = torch.randn(nb_samples, 512, device=device)\n",
        "with torch.no_grad():\n",
        "    imgs = g_all(latents)\n",
        "    imgs = (imgs.clamp(-1, 1) + 1) / 2.0 # normalization to 0..1 range\n",
        "imgs = imgs.cpu()\n",
        "\n",
        "imgs = torchvision.utils.make_grid(imgs, nrow=nb_cols)\n",
        "\n",
        "pyplot.figure(figsize=(15, 6))\n",
        "pyplot.imshow(imgs.permute(1, 2, 0).detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_rLXdUWeUPn"
      },
      "source": [
        "# Interpolation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISrMtDKSeUPn"
      },
      "outputs": [],
      "source": [
        "# Get a few Images\n",
        "nb_latents = 25\n",
        "nb_interp = 10\n",
        "fixed_latents = [torch.randn(1, 512, device=device) for _ in range(nb_interp)]\n",
        "latents = []\n",
        "for i in range(len(fixed_latents)-1):\n",
        "    latents.append(fixed_latents[i] + (fixed_latents[i + 1] - fixed_latents[i]) * torch.arange(0, 1, 0.1, device=device).unsqueeze(1))\n",
        "latents.append(fixed_latents[-1])\n",
        "latents = torch.cat(latents, dim=0)\n",
        "\n",
        "%matplotlib inline\n",
        "with torch.no_grad():\n",
        "    for latent in latents:\n",
        "        latent = latent.to(device)\n",
        "        img = g_all(latent.unsqueeze(0))\n",
        "        img = img.clamp_(-1, 1).add_(1).div_(2.0)\n",
        "        img = img.detach().squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "        pyplot.imshow(img)\n",
        "        IPython.display.clear_output(True)\n",
        "        pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7IQkjV03iVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}